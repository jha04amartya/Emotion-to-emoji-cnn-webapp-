{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NIKKS1234/emotion_to_emoji/blob/main/openCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309c298e",
      "metadata": {
        "id": "309c298e"
      },
      "outputs": [],
      "source": [
        "#pip install flask\n",
        "#uncomment the above line to install flask framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb431c51",
      "metadata": {
        "id": "cb431c51"
      },
      "outputs": [],
      "source": [
        "#importing all the important libraries for our code\n",
        "from flask import Flask, render_template, Response\n",
        "import tkinter as tk\n",
        "from tkinter import *\n",
        "from PIL import Image, ImageTk\n",
        "import numpy as np\n",
        "import random \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob as gb\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
        "from keras.models import Model,Sequential\n",
        "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
        "import time\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading our saved model\n",
        "emotion_model = load_model(r'\n",
        "# ADD PATH OF SAVED MODEL 'research50.h5' AFTER DOWNLOADING\n",
        "')\n",
        "# if you want vgg16 model then uncomment below \n",
        "#emotion_model = load_model(r'C:\\Users\\nikks\\Downloads\\model.h5')"
      ],
      "metadata": {
        "id": "kUKSGIZHRpOG"
      },
      "id": "kUKSGIZHRpOG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d69d4a3",
      "metadata": {
        "id": "8d69d4a3"
      },
      "outputs": [],
      "source": [
        "# initializing flask application\n",
        "app=Flask(__name__)\n",
        "\n",
        "# getting camera view\n",
        "cap=cv2.VideoCapture(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e21ce9",
      "metadata": {
        "id": "80e21ce9"
      },
      "outputs": [],
      "source": [
        "#defining function that returns stream of bytes of frame to flask call\n",
        "def gen():\n",
        "    cv2.ocl.setUseOpenCL(False)\n",
        "    #emotion dictionary with key as the last layer of deep neural network, and value as the corresponding emotion\n",
        "    #emotion_dict={0:\"Angry\", 1:\"Disgusted\", 2:\"Fearful\", 3:\"Happy\", 4:\"Neutral\", 5:\"Sad\", 6:\"Surprised\"}\n",
        "\n",
        "    #UPDATE PATH OF EMOJIS AFTER DOWNLOADING THE EMOJI FOLDER\n",
        "\n",
        "    emotion_dict={0:r\"\\angry.jpg\", 1:r\"\\disgusted.jpg\", 2:r\"\\fearful.jpg\", 3:r\"\\happy.jpg\", 4:r\"\\neutral.jpg\", 5:r\"\\sad.jpg\", 6:r\"\\surprise.jpg\"}\n",
        "\n",
        "    while True:\n",
        "        ret, frame= cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "            \n",
        "        bounding_box= cv2.CascadeClassifier(r'C:\\Python310\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml')\n",
        "        gray_frame=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        num_faces=bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "        for (x, y, w, h) in num_faces:\n",
        "            cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "            roi_gray_frame=gray_frame[y:y+h, x:x+w]\n",
        "            cropped_img=np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)), -1), 0)\n",
        "\n",
        "            #getting array of seven values, each value representing relative probability of a particular emotion of being the result\n",
        "            emotion_prediction=emotion_model.predict(cropped_img)\n",
        "\n",
        "            #getting the index of maximum probable emotion\n",
        "            maxindex=int(np.argmax(emotion_prediction))\n",
        "\n",
        "            #reading the emoji corresponding to maxindex from emotion dictionary\n",
        "            em_img=cv2.imread(emotion_dict[maxindex])\n",
        "\n",
        "            #getting dimensions of image and adding to the video frame\n",
        "            img_height,img_w,_=em_img.shape\n",
        "            y=y-50\n",
        "            frame[y:y+img_height,x:x+img_w]=em_img\n",
        "\n",
        "            #breking the video input frame to bytes of information to be returned to flask call\n",
        "            ret,buffer=cv2.imencode('.jpg',frame)\n",
        "            frame=buffer.tobytes()\n",
        "            yield (b'--frame\\r\\n'\n",
        "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
        "            # to get stable result giving a time lag of .1 second\n",
        "            time.sleep(.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1de54c0",
      "metadata": {
        "id": "c1de54c0"
      },
      "outputs": [],
      "source": [
        "@app.route('/')\n",
        "def home():\n",
        "  #error handling if the homepage of html file is not shown up\n",
        "    try:\n",
        "        return render_template(r'\n",
        "              #  ADD PATH OF 'main.html' FILE AFTER DOWNLOADING\n",
        "        ')#render template helps in showing a separate html file in flask\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "@app.route('/d')\n",
        "def d():\n",
        "    return Response(gen(), mimetype='multipart/x-mixed-replace; boundary=frame')# Response helps in returning stream of bytes of frame\n",
        "if __name__==\"__main__\":\n",
        "  #running application\n",
        "    Flask.run(app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4c2a65",
      "metadata": {
        "id": "af4c2a65"
      },
      "outputs": [],
      "source": [
        "#code to check emotion to text\n",
        "\n",
        "#  cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "# emotion_dict={0:\"Angry\", 1:\"Disgusted\", 2:\"Fearful\", 3:\"Happy\", 4:\"Neutral\", 5:\"Sad\", 6:\"Surprised\"}\n",
        "\n",
        "#  cap=cv2.VideoCapture(0)\n",
        "\n",
        "#  while True:\n",
        "#      ret, frame= cap.read()\n",
        "#      if not ret:\n",
        "#          break\n",
        "#      bounding_box= cv2.CascadeClassifier(r'C:\\Python310\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml')\n",
        "#      gray_frame=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "#      num_faces=bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "#      for (x, y, w, h) in num_faces:\n",
        "#          cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "#          roi_gray_frame=gray_frame[y:y+h, x:x+w]\n",
        "#          cropped_img=np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)), -1), 0)\n",
        "#          emotion_prediction=emotion_model.predict(cropped_img)\n",
        "#          maxindex=int(np.argmax(emotion_prediction))\n",
        "#          cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "#      cv2.imshow('Video', cv2.resize(frame, (1200, 860), interpolation=cv2.INTER_CUBIC))\n",
        "#      if cv2.waitKey(1)&0xFF==ord('q'):\n",
        "#          cap.release()\n",
        "#          cv2.destroyAllWindows()\n",
        "#          break;"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "test (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}